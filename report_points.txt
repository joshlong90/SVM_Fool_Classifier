Report

Understanding the data:
 - We first tried to figure out what the two classes represent. 
 - To our surprise, they appeared to be indistinguishable to human eyes.
 - We ran some initial tests to determine the relative frequency of words in each class, the results did not provide any meaningful information to assist us in determining the classes.
 - At this point we speculated that each class may in fact be slightly different distributions of words that do not fall into any natural categories.

Training a classifier:
 - Our first goal was to train an accurate local classifier to be used for information extraction and local testing.
 - We learned how to implement a SVM classifier using sklearn.
 - The first approach was to use CountVectorizer with a linear kernel on its default settings.
 - this approach when using the 540 provided training examples proved to not give accuracy any better than chance when tested on the test_data.txt file.

- We realized that in order to evaluate the accuracy of our classifier, we would need separate training and test datasets. We were only given a test file for class 1, but we had twice as many training examples for class 0. We therefore used half the examples in class-0.txt for training, and the other half for testing. This gave us 180 training examples for each class, 180 test examples for class 0, and 200 test examples for class 1.

 - After further research, we learned about tf-idf (text frequency / inverse document frequency), which improved the classifier. We were able to achieve accuracy percentages in the high sixties. This seemed intuitive at this point as by applying tf-idf we were assigning much less value to words such as "the" which appear in every training example. We now at least knew that there was a detectable statistical difference between the classes, although we still could not determine what they represented.

 - From this point we did some experimentation to improve our classifier accuracy with different kernels and performed a grid search to find optimal parameters. However, this didn't yield much improvement. With a linear kernel, we were unable to improve on the default parameters. Our best polynomial kernel was only slightly better.

Fooling the target classifier

Having trained a classifer, the next task was to find a way to extract useful information from it. We learned that it is easy to extract feature weights from a linear SVM. These weights correspond to how much importance the SVM assigns to each feature (word) when deciding on the class. A large positive weight means that the word strongly suggests class 1, and a large negative weight strongly suggests class 0. A weight near 0 means that the word has little effect on the classification.

Our first idea to fool the target classifier was simply to remove the 10 words in each example that most strongly suggest class 1, and add the 10 words which most strongly suggest class 0. Our algorithm worked like this:

- Train a classifier on the examples in class-0.txt and class-1.txt.
- Extract the weights that the classifier assigned to each word.
- Create a list of all the class 0 words, sorted in decreasing order of the absolute value of their weight. i.e. the words that most strongly suggest class 0 are at the start of the list.
- Create a dictionary mapping each class 1 word to an integer 'importance' rating. A low rating means high importance, so that when we sort by rating, the most important words come first.
- For each example in test_data.txt, find the 10 most important class 1 words and remove them, then add the 10 most important class 0 words that are not already present.

This worked well when tested against our own classifier. We realized that the target classifier would be trained against more examples, and would differ from ours in unknown ways, and would therefore assign different importances to each word. However, we hoped that it would be similar enough for our algorithm to be fairly effective.

Submission Attempts:
 - Our first submission attempt using the above algorithm achieved 48% accuracy.

 - We next decided to sort the class 1 and class 0 words within one list and sorted according to the absolute value of their weights.
 - This had the ultimate outcome of adding many more words rather than swapping. 
 - The theory was to add only the most important words and we would only remove a word if it had a higher value than one of the top 20 class 0 words.
 - This approach looked promising against our own classifier, but when we submited we only achieved 18% accuracy.

 - Our next attempt went back to swapping words, but used a TfidfVectorizer with binary=True rather than tfidf.
 - Our accuracy was 33.5%, worse than our first attempt, so we decided that this was a dead end.

 - Our fourth attempt made a number of changes. We noticed that the CountVectorizer removed puntuation, e.g. (mr. changed to mr).
 - Since the word "mr" had a high weight in favour of class 1, we thought it might be better to leave punctuation in place. We read the documentation on CountVectorizer, and learned that you can use a custom tokenizer to change its default behaviour. We modifed our code to use a custom tokenizer which only splits on spaces, and does not remove punctuation. Interestingly, this showed that parentheses were one of the most important features for distinguishing the classes. We verified that parentheses are about twice as frequent in class 0 as class 1. This change also improved the accuracy of our own classifier to around 70%.
 - We also revisited the observation that adding more words than removing had produced poor results, and decided to try only removing words.
 - Our accuracy improved to 92%.

 - After this attempt we wanted to verify whether or not tokens such as "(", ")", and "," are useful for classification.
 - Our next attempt removed these tokens from being considered for removal.
 - This caused our success to fall to 78.5%, showing that punctuation seems to be one of the important differences between the classes.

 - From here we wanted to test whether an optimal ratio of removed and added words could be found.
 - We wondered if adding a few words might be better than only removing words. 
 - We tried adding 2 words and removing 18. 
 - This caused our accuracy to drop slightly to 91%.
